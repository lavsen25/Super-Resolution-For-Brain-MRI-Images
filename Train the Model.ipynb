{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.engine import Input, Model\n",
    "from keras.layers import Conv3D, Activation, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import os\n",
    "patch_size =64\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_fspecial_gauss(size, sigma):\n",
    "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
    "    \"\"\"\n",
    "    x_data, y_data = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "\n",
    "    x_data = np.expand_dims(x_data, axis=-1)\n",
    "    x_data = np.expand_dims(x_data, axis=-1)\n",
    "\n",
    "    y_data = np.expand_dims(y_data, axis=-1)\n",
    "    y_data = np.expand_dims(y_data, axis=-1)\n",
    "\n",
    "    x = tf.constant(x_data, dtype=tf.float32)\n",
    "    y = tf.constant(y_data, dtype=tf.float32)\n",
    "\n",
    "    g = tf.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "    return g / tf.reduce_sum(g)\n",
    "\n",
    "\n",
    "def ssim (y_true, y_pred):\n",
    "    return tf_ssim(y_true, y_pred, cs_map=False, mean_metric=True, size=11, sigma=1.5)\n",
    "def tf_ssim(img1, img2, cs_map=False, mean_metric=True, size=11, sigma=1.5):\n",
    "    img1 = tf.reshape(img1, [1, 64, -1, 1])\n",
    "    img2 = tf.reshape(img2, [1, 64, -1, 1]) \n",
    "    \n",
    "    window = _tf_fspecial_gauss(size, sigma) # window shape [size, size]\n",
    "    K1 = 0.01\n",
    "    K2 = 0.03\n",
    "    L = 1  # depth of image (255 in case the image has a differnt scale)\n",
    "    C1 = (K1*L)**2\n",
    "    C2 = (K2*L)**2\n",
    "    mu1 = tf.nn.conv2d(img1, window, strides=[1,1,1,1], padding='VALID')\n",
    "    mu2 = tf.nn.conv2d(img2, window, strides=[1,1,1,1],padding='VALID')\n",
    "    mu1_sq = mu1*mu1\n",
    "    mu2_sq = mu2*mu2\n",
    "    mu1_mu2 = mu1*mu2\n",
    "    sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n",
    "    sigma2_sq = tf.nn.conv2d(img2*img2, window, strides=[1,1,1,1],padding='VALID') - mu2_sq\n",
    "    sigma12 = tf.nn.conv2d(img1*img2, window, strides=[1,1,1,1],padding='VALID') - mu1_mu2\n",
    "    if cs_map:\n",
    "        value = (((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
    "                    (sigma1_sq + sigma2_sq + C2)),\n",
    "                (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2))\n",
    "    else:\n",
    "        value = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
    "                    (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if mean_metric:\n",
    "        value = tf.reduce_mean(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "class LeakyReLU(LeakyReLU):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__name__ = \"LeakyReLU\"\n",
    "        super(LeakyReLU, self).__init__(**kwargs)  \n",
    "\n",
    "\n",
    "def mean_sq_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    return tf_psnr(y_true, y_pred, max_val=1.0, name=None)\n",
    "def tf_psnr(a, b, max_val, name=None):\n",
    "    \"\"\"Returns the Peak Signal-to-Noise Ratio between a and b.\n",
    "    This is intended to be used on signals (or images). Produces a PSNR value for\n",
    "    each image in batch.\n",
    "    The last three dimensions of input are expected to be [height, width, depth].\n",
    "    Example:\n",
    "    ```python\n",
    "    # Read images from file.\n",
    "    im1 = tf.decode_png('path/to/im1.png')\n",
    "    im2 = tf.decode_png('path/to/im2.png')\n",
    "    # Compute PSNR over tf.uint8 Tensors.\n",
    "    psnr1 = tf.image.psnr(im1, im2, max_val=255)\n",
    "    # Compute PSNR over tf.float32 Tensors.\n",
    "    im1 = tf.image.convert_image_dtype(im1, tf.float32)\n",
    "    im2 = tf.image.convert_image_dtype(im2, tf.float32)\n",
    "    psnr2 = tf.image.psnr(im1, im2, max_val=1.0)\n",
    "    # psnr1 and psnr2 both have type tf.float32 and are almost equal.\n",
    "    ```\n",
    "    Arguments:\n",
    "    a: First set of images.\n",
    "    b: Second set of images.\n",
    "    max_val: The dynamic range of the images (i.e., the difference between the\n",
    "    maximum the and minimum allowed values).\n",
    "    name: Namespace to embed the computation in.\n",
    "    Returns:\n",
    "    The scalar PSNR between a and b. The returned tensor has type `tf.float32`\n",
    "    and shape [batch_size, 1].\n",
    "    \"\"\"\n",
    "    with ops.name_scope(name, 'PSNR', [a, b]):\n",
    "    # Need to convert the images to float32.  Scale max_val accordingly so that\n",
    "    # PSNR is computed correctly.\n",
    "        max_val = math_ops.cast(max_val, a.dtype)\n",
    "#         max_val = convert_image_dtype(max_val, dtypes.float32)\n",
    "#         a = convert_image_dtype(a, dtypes.float32)\n",
    "#         b = convert_image_dtype(b, dtypes.float32)\n",
    "        mse = math_ops.reduce_mean(math_ops.squared_difference(a, b), [-4, -3, -2])\n",
    "        psnr_val = math_ops.subtract(\n",
    "        20 * math_ops.log(max_val) / math_ops.log(10.0),\n",
    "        np.float64(10 / np.log(10)) * math_ops.log(mse),\n",
    "        name='psnr')\n",
    "\n",
    "#         _, _, checks = _verify_compatible_image_shapes(a, b)\n",
    "#         with ops.control_dependencies(checks):\n",
    "        return array_ops.identity(psnr_val)\n",
    "        \n",
    "def DenseNet(patch_size=64, growth_rate=24):\n",
    "    n_channels=growth_rate\n",
    "    input_shape=(patch_size, patch_size, patch_size, 1)\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    #Initial Convolution Layer\n",
    "    x = Conv3D(filters=2*growth_rate,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(inputs)\n",
    "\n",
    "    no_layers = 4\n",
    "    for i in range(no_layers):\n",
    "        x_list = [x]\n",
    "        cb = Conv3D(filters=2*growth_rate,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(x)\n",
    "        x_list.append(cb)\n",
    "        x = Concatenate(axis=-1)(x_list)\n",
    "        \n",
    "        n_channels += growth_rate\n",
    "        # for transititon layer\n",
    "        x = Conv3D( n_channels,kernel_size=(1, 1, 1), padding='same', activation = LeakyReLU(alpha=0.1)) (x)\n",
    "        \n",
    "\n",
    "    x = Conv3D( 1,kernel_size=(3, 3, 3), padding='same') (x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    adamOpt = Adam(lr=0.00001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adamOpt,metrics=[mean_sq_error, psnr, ssim ])\n",
    "    model.summary(line_length=110)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerator = tf.log(tf.constant(0.0190))\n",
    "# # denom = tf.log(tf.constant(10, dtype=tf.float64))# psnr_random(10)\n",
    "# # res = numerator / denom\n",
    "# # init_op = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op) #execute init_op\n",
    "#     #print the random values that we sample\n",
    "#     print (sess.run(numerator))\n",
    "\n",
    "# print( np.log(0.0190))\n",
    "# print( np.log(0.0190) / np.log(10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________________________\n",
      "Layer (type)                        Output Shape            Param #      Connected to                         \n",
      "==============================================================================================================\n",
      "input_2 (InputLayer)                (None, 64, 64, 64, 1)   0                                                 \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)                  (None, 64, 64, 64, 48)  1344         input_2[0][0]                        \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)                  (None, 64, 64, 64, 48)  62256        conv3d_11[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)         (None, 64, 64, 64, 96)  0            conv3d_11[0][0]                      \n",
      "                                                                         conv3d_12[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)                  (None, 64, 64, 64, 48)  4656         concatenate_5[0][0]                  \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)                  (None, 64, 64, 64, 48)  62256        conv3d_13[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)         (None, 64, 64, 64, 96)  0            conv3d_13[0][0]                      \n",
      "                                                                         conv3d_14[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)                  (None, 64, 64, 64, 72)  6984         concatenate_6[0][0]                  \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)                  (None, 64, 64, 64, 48)  93360        conv3d_15[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)         (None, 64, 64, 64, 120) 0            conv3d_15[0][0]                      \n",
      "                                                                         conv3d_16[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)                  (None, 64, 64, 64, 96)  11616        concatenate_7[0][0]                  \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)                  (None, 64, 64, 64, 48)  124464       conv3d_17[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)         (None, 64, 64, 64, 144) 0            conv3d_17[0][0]                      \n",
      "                                                                         conv3d_18[0][0]                      \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)                  (None, 64, 64, 64, 120) 17400        concatenate_8[0][0]                  \n",
      "______________________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)                  (None, 64, 64, 64, 1)   3241         conv3d_19[0][0]                      \n",
      "==============================================================================================================\n",
      "Total params: 387,577\n",
      "Trainable params: 387,577\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=DenseNet(patch_size=64, growth_rate=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Loading and preprocessing train data 64x64x64 Patch Size..\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "patch_size=64\n",
    "from keras.utils import plot_model\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "import time\n",
    "train_batch_size = 1\n",
    "reduceLearningRate  = 0.5\n",
    "\n",
    "\n",
    "print('-'*60)\n",
    "print('Loading and preprocessing train data 64x64x64 Patch Size..')\n",
    "print('-'*60)\n",
    "trainImg = np.load('patches3D/patchesStandardized/patchesTrainImgLR.npy')\n",
    "trainGt = np.load('patches3D/patchesStandardized/patchesTrainImgHR.npy')\n",
    "\n",
    "print('-'*60)\n",
    "print('Loading and preprocessing validation data 64x64x64 Patch Size..')\n",
    "print('-'*60)\n",
    "valImg = np.load('patches3D/patchesStandardized/patchesvalImgLR.npy')\n",
    "valGt = np.load('patches3D/patchesStandardized/patchesvalImgHR.npy')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('-'*60)\n",
    "print('Fitting model...')\n",
    "print('-'*60)\n",
    "\n",
    "#============================================================================\n",
    "print('training starting..')\n",
    "\n",
    "if 'outputs' not in os.listdir(os.curdir):\n",
    "    os.mkdir('outputs')\n",
    "\n",
    "\n",
    "log_filename = 'outputs/' + '3dPatch' +'_model_train.csv'\n",
    "\n",
    "csv_log = callbacks.CSVLogger(log_filename, separator=',', append=True)\n",
    "\n",
    "checkpoint_filepath = 'outputs/' + 'model-{epoch:03d}.h5'\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [csv_log, checkpoint]\n",
    "callbacks_list.append(ReduceLROnPlateau(factor=reduceLearningRate, patience=3,\n",
    "                                           verbose=True))\n",
    "callbacks_list.append(EarlyStopping(verbose=True, patience=3))\n",
    "\n",
    "#============================================================================\n",
    "history = model.fit(trainImg, trainGt, epochs=2, verbose=1, batch_size=train_batch_size , validation_data=(valImg,valGt), shuffle=True, callbacks=callbacks_list) \n",
    "\n",
    "model_name = 'outputs/' + '3dPatch64' + '_model_last'\n",
    "model.save(model_name)  # creates a HDF5 file 'my_model.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (np.max(trainImg))\n",
    "print (np.max(trainGt))\n",
    "\n",
    "# print (np.max(valImg))\n",
    "# print (np.max(valGt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Activation, Convolution3D, Dropout, GlobalAveragePooling3D, Concatenate, Dense, Input, AveragePooling3D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.regularizers import l2\n",
    "\n",
    "# def DenseNet(\n",
    "#     input_shape=None,\n",
    "#     dense_blocks=4,\n",
    "#     dense_layers=-1,\n",
    "#     growth_rate=24,\n",
    "#     nb_classes=None,\n",
    "#     dropout_rate=None,\n",
    "#     bottleneck=False,\n",
    "#     compression=1.0,\n",
    "#     weight_decay=1e-4,\n",
    "#     depth=40):\n",
    "#     \"\"\"\n",
    "#     Creating a DenseNet\n",
    "    \n",
    "#     Arguments:\n",
    "#         input_shape  : shape of the input images. E.g. (28,28,1) for MNIST    \n",
    "#         dense_blocks : amount of dense blocks that will be created (default: 3)    \n",
    "#         dense_layers : number of layers in each dense block. You can also use a list for numbers of layers [2,4,3]\n",
    "#                        or define only 2 to add 2 layers at all dense blocks. -1 means that dense_layers will be calculated\n",
    "#                        by the given depth (default: -1)\n",
    "#         growth_rate  : number of filters to add per dense block (default: 12)\n",
    "#         nb_classes   : number of classes\n",
    "#         dropout_rate : defines the dropout rate that is accomplished after each conv layer (except the first one).\n",
    "#                        In the paper the authors recommend a dropout of 0.2 (default: None)\n",
    "#         bottleneck   : (True / False) if true it will be added in convolution block (default: False)\n",
    "#         compression  : reduce the number of feature-maps at transition layer. In the paper the authors recomment a compression\n",
    "#                        of 0.5 (default: 1.0 - will have no compression effect)\n",
    "#         weight_decay : weight decay of L2 regularization on weights (default: 1e-4)\n",
    "#         depth        : number or layers (default: 40)\n",
    "        \n",
    "#     Returns:\n",
    "#         Model        : A Keras model instance\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if compression <=0.0 or compression > 1.0:\n",
    "#         raise Exception('Compression have to be a value between 0.0 and 1.0.')\n",
    "    \n",
    "#     if type(dense_layers) is list:\n",
    "#         if len(dense_layers) != dense_blocks:\n",
    "#             raise AssertionError('Number of dense blocks have to be same length to specified layers')\n",
    "#     elif dense_layers == -1:\n",
    "#         dense_layers = int((depth - 4)/3)\n",
    "#         if bottleneck:\n",
    "#             dense_layers = int(dense_layers / 2)\n",
    "#         dense_layers = [dense_layers for _ in range(dense_blocks)]\n",
    "#     else:\n",
    "#         dense_layers = [dense_layers for _ in range(dense_blocks)]\n",
    "        \n",
    "#     img_input = Input(shape=input_shape)\n",
    "#     nb_channels = growth_rate\n",
    "    \n",
    "    \n",
    "#     print('#############################################')\n",
    "#     print('Dense blocks: %s' % dense_blocks)\n",
    "#     print('Layers per dense block: %s' % dense_layers)\n",
    "#     print('#############################################')\n",
    "    \n",
    "#     # Initial convolution layer\n",
    "#     x = Convolution3D(2 * growth_rate, (3,3,3), padding='same',strides=(1,1,1),\n",
    "#                       use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
    "    \n",
    "#     # Building dense blocks\n",
    "#     for block in range(dense_blocks - 1):\n",
    "        \n",
    "#         # Add dense block\n",
    "#         x, nb_channels = dense_block(x, dense_layers[block], nb_channels, growth_rate, dropout_rate, bottleneck, weight_decay)\n",
    "        \n",
    "#         # Add transition_block\n",
    "#         x = transition_layer(x, nb_channels, dropout_rate, compression, weight_decay)\n",
    "#         nb_channels = int(nb_channels * compression)\n",
    "    \n",
    "#     # Add last dense block without transition but for that with global average pooling\n",
    "#     x, nb_channels = dense_block(x, dense_layers[-1], nb_channels, growth_rate, dropout_rate, weight_decay)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = GlobalAveragePooling3D()(x)\n",
    "    \n",
    "#     x = Dense(1)(x)\n",
    "    \n",
    "#     return Model(img_input, x, name='densenet')\n",
    "\n",
    "\n",
    "# def dense_block(x, nb_layers, nb_channels, growth_rate, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
    "#     \"\"\"\n",
    "#     Creates a dense block and concatenates inputs\n",
    "#     \"\"\"\n",
    "    \n",
    "#     x_list = [x]\n",
    "#     for i in range(nb_layers):\n",
    "#         cb = convolution_block(x, growth_rate, dropout_rate, bottleneck)\n",
    "#         x_list.append(cb)\n",
    "#         x = Concatenate(axis=-1)(x_list)\n",
    "#         nb_channels += growth_rate\n",
    "#     return x, nb_channels\n",
    "\n",
    "\n",
    "# def convolution_block(x, nb_channels, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
    "#     \"\"\"\n",
    "#     Creates a convolution block consisting of BN-ReLU-Conv.\n",
    "#     Optional: bottleneck, dropout\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Bottleneck\n",
    "#     if bottleneck:\n",
    "#         bottleneckWidth = 4\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Convolution3D(nb_channels * bottleneckWidth, (1, 1,1), use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "#         # Dropout\n",
    "#         if dropout_rate:\n",
    "#             x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "#     # Standard (BN-ReLU-Conv)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Convolution3D(nb_channels, (3,3, 3), padding='same', use_bias=False)(x)\n",
    "    \n",
    "#     # Dropout\n",
    "#     if dropout_rate:\n",
    "#         x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "#     return x\n",
    "\n",
    "\n",
    "# def transition_layer(x, nb_channels, dropout_rate=None, compression=1.0, weight_decay=1e-4):\n",
    "#     \"\"\"\n",
    "#     Creates a transition layer between dense blocks as transition, which do convolution and pooling.\n",
    "#     Works as downsampling.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Convolution3D(int(nb_channels*compression), (1, 1,1), padding='same',\n",
    "#                       use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "#     # Adding dropout\n",
    "#     if dropout_rate:\n",
    "#         x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "#     x = AveragePooling3D((3, 3,3), strides=(2, 2,2))(x)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Variables: name (type shape) [size]\n",
    "# # ---------\n",
    "# # Variable:0 (float32_ref 3x3x3x1x48) [1296, bytes: 5184]\n",
    "# # Variable_1:0 (float32_ref 3x3x3x48x24) [31104, bytes: 124416]\n",
    "# # Variable_2:0 (float32_ref 3x3x3x72x24) [46656, bytes: 186624]\n",
    "# # Variable_3:0 (float32_ref 3x3x3x96x24) [62208, bytes: 248832]\n",
    "# # Variable_4:0 (float32_ref 3x3x3x120x24) [77760, bytes: 311040]\n",
    "# # Variable_5:0 (float32_ref 3x3x3x144x1) [3888, bytes: 15552]\n",
    "# # Total size of variables: 222912\n",
    "# # Total bytes of variables: 891648\n",
    "    \n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "# class LeakyReLU(LeakyReLU):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.__name__ = \"LeakyReLU\"\n",
    "#         super(LeakyReLU, self).__init__(**kwargs)  \n",
    "  \n",
    "\n",
    "    \n",
    "# #     for i in range(nb_layers):\n",
    "# #         cb = convolution_block(x, growth_rate, dropout_rate, bottleneck)\n",
    "# #         x_list.append(cb)\n",
    "# #         x = Concatenate(axis=-1)(x_list)\n",
    "# #         nb_channels += growth_rate\n",
    "# #     return x, nb_channels\n",
    "\n",
    "# def DenseNet(input_shape=(patch_size, patch_size, patch_size, 1)):\n",
    "   \n",
    "#     growth_rate=24\n",
    "#     inputs = Input(input_shape)\n",
    "#     #Initial Convolution Block\n",
    "#     x = Conv3D(filters=growth_rate*2,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(inputs)\n",
    "#     print (x.shape)\n",
    "#     x_list = [x]\n",
    "#     x_list.append(x)\n",
    "#     x = Concatenate(axis=-1)(x_list)\n",
    "#     print (x.shape)\n",
    "    \n",
    "#     x= Conv3D(filters=growth_rate,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(x)\n",
    "#     x_list.append(x)\n",
    "#     x = Concatenate(axis=-1)(x_list)   \n",
    "    \n",
    "#     x = Conv3D(filters=growth_rate,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(x)\n",
    "#     x_list.append(x)\n",
    "#     x = Concatenate(axis=-1)(x_list)  \n",
    "    \n",
    "#     x= Conv3D(filters=growth_rate,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(x)\n",
    "#     x_list.append(x)\n",
    "#     x = Concatenate(axis=-1)(x_list)   \n",
    "    \n",
    "#     x= Conv3D(filters=growth_rate,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(x)\n",
    "#     x_list.append(x)\n",
    "#     x = Concatenate(axis=-1)(x_list)   \n",
    "    \n",
    "#     x = Conv3D(filters=1,  kernel_size=(3, 3, 3), padding='same')(x)\n",
    "#     x_list.append(x)\n",
    "#     x = Concatenate(axis=-1)(x_list)   \n",
    "    \n",
    "                  \n",
    "#     model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "#     adamOpt = Adam(lr=0.00001)\n",
    "#     model.compile(loss='mean_squared_error', optimizer=adamOpt)\n",
    "#     return model\n",
    "# #     conv7 Conv3D(filters=growth_rate,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(x)\n",
    "# #     x_list.append(conv7)\n",
    "# #     x = Concatenate(axis=-1)(x_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "# class LeakyReLU(LeakyReLU):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.__name__ = \"LeakyReLU\"\n",
    "#         super(LeakyReLU, self).__init__(**kwargs)\n",
    "\n",
    "# def convNetBaseLine(input_shape=(patch_size, patch_size, patch_size, 1)):\n",
    "\n",
    "#     inputs = Input(input_shape)\n",
    "\n",
    "#     conv1= Conv3D(filters=16,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(inputs)\n",
    "#     conv1= Conv3D(filters=16,  kernel_size=(3, 3, 3), padding='same')(conv1)\n",
    "#     conv1= Activation(LeakyReLU(alpha=0.1))(conv1)\n",
    "   \n",
    "#     conv2= Conv3D(filters=32,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(conv1)\n",
    "#     conv2= Conv3D(filters=32,  kernel_size=(3, 3, 3), padding='same')(conv2)\n",
    "#     conv2= Activation(LeakyReLU(alpha=0.1))(conv2)\n",
    "    \n",
    "#     conv3= Conv3D(filters=64,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(conv2)\n",
    "#     conv3= Conv3D(filters=64,  kernel_size=(3, 3, 3), padding='same')(conv3)\n",
    "#     conv3= Activation(LeakyReLU(alpha=0.1))(conv3)\n",
    "\n",
    "    \n",
    "#     conv4=Conv3D(filters=128,  kernel_size=(3, 3, 3), padding='same')(conv3)\n",
    "#     conv4=Conv3D(filters=128,  kernel_size=(3, 3, 3), padding='same')(conv4)\n",
    "#     conv4= Activation(LeakyReLU(alpha=0.1))(conv4)\n",
    "\n",
    "\n",
    "\n",
    "#     conv5= Conv3D(filters=64,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(conv4)\n",
    "#     conv5= Conv3D(filters=64,  kernel_size=(3, 3, 3), padding='same')(conv5)\n",
    "#     conv5= Activation(LeakyReLU(alpha=0.1))(conv5)\n",
    "   \n",
    "#     conv6= Conv3D(filters=32,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(conv5)\n",
    "#     conv6= Conv3D(filters=32,  kernel_size=(3, 3, 3), padding='same')(conv6)\n",
    "#     conv6= Activation(LeakyReLU(alpha=0.1))(conv6)\n",
    "    \n",
    "#     conv7= Conv3D(filters=16,  kernel_size=(3, 3, 3), padding='same', activation = LeakyReLU(alpha=0.1))(conv6)\n",
    "#     conv7= Conv3D(filters=16,  kernel_size=(3, 3, 3), padding='same')(conv7)\n",
    "#     conv7= Activation(LeakyReLU(alpha=0.1))(conv7)\n",
    "    \n",
    "#     conv8= Conv3D(filters=1,  kernel_size=(3, 3, 3), padding='same')(conv7)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=conv8)\n",
    "\n",
    "#     adamOpt = Adam(lr=0.00001)\n",
    "#     model.compile(loss='mean_squared_error', optimizer=adamOpt)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr_random(x):\n",
    "        return -10. * compute_logb10(x) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logb10(x):\n",
    "        numerator = tf.log(x)\n",
    "        denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "\n",
    "\n",
    "model = DenseNet(\n",
    "    input_shape=(patch_size, patch_size, patch_size, 1),\n",
    "    dense_blocks=4,\n",
    "    dense_layers=-1,\n",
    "    growth_rate=24,\n",
    "    nb_classes=None,\n",
    "    dropout_rate=None,\n",
    "    bottleneck=False,\n",
    "    compression=1.0,\n",
    "    weight_decay=1e-4,\n",
    "    depth=10)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "print('-'*60)\n",
    "print('Fitting model...')\n",
    "print('-'*60)\n",
    "\n",
    "#============================================================================\n",
    "print('training starting..')\n",
    "\n",
    "if 'outputs' not in os.listdir(os.curdir):\n",
    "    os.mkdir('outputs')\n",
    "\n",
    "\n",
    "log_filename = 'outputs/' + '3dPatch' +'_model_train.csv'\n",
    "\n",
    "csv_log = callbacks.CSVLogger(log_filename, separator=',', append=True)\n",
    "\n",
    "checkpoint_filepath = 'outputs/' + 'model-{epoch:03d}.h5'\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [csv_log, checkpoint]\n",
    "callbacks_list.append(ReduceLROnPlateau(factor=reduceLearningRate, patience=3,\n",
    "                                           verbose=True))\n",
    "callbacks_list.append(EarlyStopping(verbose=True, patience=3))\n",
    "\n",
    "#============================================================================\n",
    "history = model.fit(trainImg, trainGt, epochs=2, verbose=1, batch_size=train_batch_size , validation_data=(valImg,valGt), shuffle=True, callbacks=callbacks_list) \n",
    "\n",
    "model_name = 'outputs/' + '3dPatch64' + '_model_last'\n",
    "model.save(model_name)  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
